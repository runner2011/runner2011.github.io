"use strict";(self.webpackChunkrunn2011blog=self.webpackChunkrunn2011blog||[]).push([[2618],{412:(s,a,t)=>{t.r(a),t.d(a,{comp:()=>l,data:()=>p});var e=t(8178);const n={},l=(0,t(628).A)(n,[["render",function(s,a){return(0,e.uX)(),(0,e.CE)("div",null,a[0]||(a[0]=[(0,e.Fv)('<h1 id="正则化-岭回归-lasso回归概念" tabindex="-1"><a class="header-anchor" href="#正则化-岭回归-lasso回归概念"><span>正则化,岭回归,lasso回归概念</span></a></h1><p>A:把我当作退休的老太太，给我解释正则化，岭回归，lasso回归。</p><p>好！我来用退休老太太能听懂的方式，慢慢给你讲讲哈。我们就当在小区凉亭里唠嗑，慢慢来：</p><hr><h2 id="🌸-一、先说说正则化是个啥" tabindex="-1"><a class="header-anchor" href="#🌸-一、先说说正则化是个啥"><span>🌸 一、先说说<strong>正则化</strong>是个啥</span></a></h2><p>你做菜放盐吧，太多太少都不好。模型也是这样，学得太少，预测不准；学得太多，连你家狗叫声的节奏都学进去，结果到新数据上就翻车，这叫<strong>过拟合</strong>。</p><p><strong>正则化</strong>就是：<br> 👉 在模型学数据的时候，旁边盯着，别让你学得太复杂了。<br> 就像做菜有人提醒：“哎哎，盐别放太多，别太咸了哈！”</p><p>用数学说法：</p><blockquote><p><strong>在原本的损失函数后面，加个惩罚项</strong>，约束模型参数（就是公式里的系数）别乱跑太大。</p></blockquote><hr><h2 id="🌸-二、岭回归-ridge-regression" tabindex="-1"><a class="header-anchor" href="#🌸-二、岭回归-ridge-regression"><span>🌸 二、岭回归（Ridge Regression）</span></a></h2><p>你家有好几个儿子，每个儿子挣得多挣得少都不同。你给他们发压岁钱，怕有个别儿子要太多。</p><p><strong>岭回归</strong>就像：<br> 👉 给每个系数（儿子）发压岁钱，要求：</p><ul><li>总金额不能太大。</li><li>谁也别太夸张。</li></ul><p>方法是：<br> 在损失函数后面加上：</p><blockquote><p><strong>所有系数的平方和 × λ</strong></p></blockquote><p>λ 是个你定的参数，λ 越大，管得越严，大家拿得都差不多，谁也别太离谱。</p><p>数学长这样：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>损失</mtext><mo>=</mo><mtext>原来的误差</mtext><mo>+</mo><mi>λ</mi><mo>×</mo><mo stretchy="false">(</mo><mtext>所有系数的平方和</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">损失 = 原来的误差 + λ \\times (所有系数的平方和) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">损失</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord cjk_fallback">原来的误差</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">λ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord cjk_fallback">所有系数的平方和</span><span class="mclose">)</span></span></span></span></span></p><p>特点：</p><ul><li>把所有系数往 0 靠，但都不会变成 0。</li><li>用来防止过拟合，尤其是特征（变量）很多的时候。</li></ul><hr><h2 id="🌸-三、lasso回归-least-absolute-shrinkage-and-selection-operator" tabindex="-1"><a class="header-anchor" href="#🌸-三、lasso回归-least-absolute-shrinkage-and-selection-operator"><span>🌸 三、Lasso回归（Least Absolute Shrinkage and Selection Operator）</span></a></h2><p>Lasso 就更狠了，不仅管你别太大，还能直接把某些儿子从压岁钱名单上划掉，不给了！</p><p>方法是：<br> 在损失函数后面加上：</p><blockquote><p><strong>所有系数的绝对值和 × λ</strong></p></blockquote><p>数学长这样：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>损失</mtext><mo>=</mo><mtext>原来的误差</mtext><mo>+</mo><mi>λ</mi><mo>×</mo><mo stretchy="false">(</mo><mtext>所有系数的绝对值和</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">损失 = 原来的误差 + λ \\times (所有系数的绝对值和) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord cjk_fallback">损失</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord cjk_fallback">原来的误差</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">λ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord cjk_fallback">所有系数的绝对值和</span><span class="mclose">)</span></span></span></span></span></p><p>特点：</p><ul><li>λ 大的时候，某些系数会被压到 0，等于删掉这个变量。</li><li>适合做<strong>特征筛选</strong>，选出对预测最有用的那几个。</li></ul><hr><h2 id="🌸-总结一下" tabindex="-1"><a class="header-anchor" href="#🌸-总结一下"><span>🌸 总结一下：</span></a></h2><table><thead><tr><th style="text-align:left;">名字</th><th style="text-align:left;">方式</th><th style="text-align:left;">作用</th></tr></thead><tbody><tr><td style="text-align:left;">正则化</td><td style="text-align:left;">惩罚项防止模型太复杂</td><td style="text-align:left;">防止过拟合</td></tr><tr><td style="text-align:left;">岭回归</td><td style="text-align:left;">加<strong>平方和</strong>惩罚</td><td style="text-align:left;">把系数都往 0 靠，但不变成 0</td></tr><tr><td style="text-align:left;">Lasso 回归</td><td style="text-align:left;">加<strong>绝对值和</strong>惩罚</td><td style="text-align:left;">有的系数直接压成 0，等于删变量</td></tr></tbody></table><hr><h2 id="🌸-最后老太太总结" tabindex="-1"><a class="header-anchor" href="#🌸-最后老太太总结"><span>🌸 最后老太太总结：</span></a></h2><p>哎呀，这不就跟我们家发压岁钱似的嘛：</p><ul><li>怕有的孩子张嘴要太多，得管着点（正则化）</li><li>谁都别要太离谱（岭回归）</li><li>有些不争气的，干脆别给了（Lasso）</li></ul><hr><p>要是你喜欢，我还能再帮你画个图，或者用生活故事多比喻几个场景 👵✨ 要不要？</p>',39)]))}]]),p=JSON.parse('{"path":"/posts/Math/%E6%AD%A3%E5%88%99%E5%8C%96_%E5%B2%AD%E5%9B%9E%E5%BD%92_lasso%E5%9B%9E%E5%BD%92.html","title":"正则化,岭回归,lasso回归概念","lang":"zh-CN","frontmatter":{"date":"2025-07-08T00:00:00.000Z","tag":["Math","AI_GEN"],"description":"正则化,岭回归,lasso回归概念 A:把我当作退休的老太太，给我解释正则化，岭回归，lasso回归。 好！我来用退休老太太能听懂的方式，慢慢给你讲讲哈。我们就当在小区凉亭里唠嗑，慢慢来： 🌸 一、先说说正则化是个啥 你做菜放盐吧，太多太少都不好。模型也是这样，学得太少，预测不准；学得太多，连你家狗叫声的节奏都学进去，结果到新数据上就翻车，这叫过拟合...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"正则化,岭回归,lasso回归概念\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-07-08T00:00:00.000Z\\",\\"dateModified\\":\\"2025-07-12T10:51:57.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Runner2011\\",\\"url\\":\\"https://runner2011.github.io\\"}]}"],["meta",{"property":"og:url","content":"https://runner2011.github.io/posts/Math/%E6%AD%A3%E5%88%99%E5%8C%96_%E5%B2%AD%E5%9B%9E%E5%BD%92_lasso%E5%9B%9E%E5%BD%92.html"}],["meta",{"property":"og:site_name","content":"Runner2011 blog"}],["meta",{"property":"og:title","content":"正则化,岭回归,lasso回归概念"}],["meta",{"property":"og:description","content":"正则化,岭回归,lasso回归概念 A:把我当作退休的老太太，给我解释正则化，岭回归，lasso回归。 好！我来用退休老太太能听懂的方式，慢慢给你讲讲哈。我们就当在小区凉亭里唠嗑，慢慢来： 🌸 一、先说说正则化是个啥 你做菜放盐吧，太多太少都不好。模型也是这样，学得太少，预测不准；学得太多，连你家狗叫声的节奏都学进去，结果到新数据上就翻车，这叫过拟合..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-07-12T10:51:57.000Z"}],["meta",{"property":"article:tag","content":"AI_GEN"}],["meta",{"property":"article:tag","content":"Math"}],["meta",{"property":"article:published_time","content":"2025-07-08T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-07-12T10:51:57.000Z"}]]},"git":{"createdTime":1752317517000,"updatedTime":1752317517000,"contributors":[{"name":"Runner2011","username":"Runner2011","email":"chenjfsea@gmail.com","commits":1,"url":"https://github.com/Runner2011"}]},"readingTime":{"minutes":2.36,"words":709},"filePathRelative":"posts/Math/正则化,岭回归,lasso回归.md","excerpt":"\\n<p>A:把我当作退休的老太太，给我解释正则化，岭回归，lasso回归。</p>\\n<p>好！我来用退休老太太能听懂的方式，慢慢给你讲讲哈。我们就当在小区凉亭里唠嗑，慢慢来：</p>\\n<hr>\\n<h2>🌸 一、先说说<strong>正则化</strong>是个啥</h2>\\n<p>你做菜放盐吧，太多太少都不好。模型也是这样，学得太少，预测不准；学得太多，连你家狗叫声的节奏都学进去，结果到新数据上就翻车，这叫<strong>过拟合</strong>。</p>\\n<p><strong>正则化</strong>就是：<br>\\n👉 在模型学数据的时候，旁边盯着，别让你学得太复杂了。<br>\\n就像做菜有人提醒：“哎哎，盐别放太多，别太咸了哈！”</p>","autoDesc":true}')},628:(s,a)=>{a.A=(s,a)=>{const t=s.__vccOpts||s;for(const[s,e]of a)t[s]=e;return t}}}]);