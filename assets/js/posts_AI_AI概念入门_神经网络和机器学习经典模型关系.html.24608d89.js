"use strict";(self.webpackChunkrunn2011blog=self.webpackChunkrunn2011blog||[]).push([[5171],{628:(t,e)=>{e.A=(t,e)=>{const n=t.__vccOpts||t;for(const[t,s]of e)n[t]=s;return n}},1792:(t,e,n)=>{n.r(e),n.d(e,{comp:()=>i,data:()=>l});var s=n(8178);const a={},i=(0,n(628).A)(a,[["render",function(t,e){return(0,s.uX)(),(0,s.CE)("div",null,e[0]||(e[0]=[(0,s.Fv)('<h1 id="神经网络和机器学习经典模型关系" tabindex="-1"><a class="header-anchor" href="#神经网络和机器学习经典模型关系"><span>神经网络和机器学习经典模型关系</span></a></h1><p>Q:比如房价和距离模型问题，可以用线性回归模型。还有一些其他的逻辑回归，决策树等模型。这些机器学习模型和神经网络的关系是什么？</p><p>Runner2011:<br> 神经网络和经典模型的关系：</p><ul><li>每一层的神经元做的事都是一个经典模型(例如逻辑回归模型)做的事</li><li>优化：经典模型直接用梯度算法。NN用的反向传播(也会用到梯度算法)。</li></ul><hr><p>很好！我来给你捋清楚<strong>机器学习模型（如线性回归、逻辑回归、决策树等）和神经网络</strong>之间的关系和区别，咱们按体系讲清楚：</p><hr><h2 id="📌-机器学习模型体系" tabindex="-1"><a class="header-anchor" href="#📌-机器学习模型体系"><span>📌 机器学习模型体系</span></a></h2><p>机器学习是一个大类，里面有很多种不同的算法模型，按类型大致分成：</p><ul><li><strong>监督学习</strong>（有标签，预测结果）</li><li><strong>无监督学习</strong>（没有标签，找结构）</li><li><strong>强化学习</strong>（通过奖励信号学习）</li></ul><p>在<strong>监督学习</strong>中，常用的模型有：</p><table><thead><tr><th style="text-align:left;">回归类</th><th style="text-align:left;">分类类</th><th style="text-align:left;">其他</th></tr></thead><tbody><tr><td style="text-align:left;">线性回归</td><td style="text-align:left;">逻辑回归</td><td style="text-align:left;">决策树</td></tr><tr><td style="text-align:left;">岭回归、Lasso回归</td><td style="text-align:left;">支持向量机（SVM）</td><td style="text-align:left;">随机森林</td></tr><tr><td style="text-align:left;">神经网络</td><td style="text-align:left;">KNN</td><td style="text-align:left;">Gradient Boosting</td></tr></tbody></table><p><strong>神经网络</strong>其实就是监督学习里的一种模型，它属于<strong>可以做回归也可以做分类的模型</strong>，而且是<strong>复杂度很高，灵活度极强</strong>的一类。</p><hr><h2 id="📌-神经网络和传统机器学习模型的关系" tabindex="-1"><a class="header-anchor" href="#📌-神经网络和传统机器学习模型的关系"><span>📌 神经网络和传统机器学习模型的关系</span></a></h2><ul><li><p>神经网络可以看作是一种<strong>特殊形式的机器学习模型</strong>，不过它<strong>结构比线性回归、决策树复杂得多</strong>，可以用来拟合高维复杂数据的非线性关系。</p></li><li><p>比如：</p><ul><li><strong>线性回归</strong>：特征值加权求和，再输出一个值，拟合的是<strong>线性关系</strong></li><li><strong>神经网络</strong>：特征经过一层层神经元的线性加权、非线性激活、再传递，能拟合<strong>复杂非线性关系</strong></li></ul></li><li><p>从实现方式看：</p><ul><li>线性回归就是神经网络的<strong>最简形式</strong>（没有隐藏层，激活函数是恒等函数）</li><li>多层感知器（MLP）就是<strong>多层的线性回归+非线性激活函数</strong></li><li>决策树、随机森林等是基于<strong>条件判断和信息熵分裂</strong>的模型，和神经网络是两种不同思想，但同属于机器学习</li></ul></li></ul><hr><h2 id="📌-举个例子总结" tabindex="-1"><a class="header-anchor" href="#📌-举个例子总结"><span>📌 举个例子总结：</span></a></h2><ul><li><p>假设你要预测房价</p><ul><li><strong>线性回归</strong>：房价 = 地段 × a + 面积 × b + 常数项 c</li><li><strong>决策树</strong>：先看地段是否好 → 再看面积是否&gt;100 → 再给出价格预测</li><li><strong>神经网络</strong>：把所有特征输入神经元，层层计算复杂关系，最后输出房价</li></ul></li></ul><hr><h2 id="📌-它们的联系和差别" tabindex="-1"><a class="header-anchor" href="#📌-它们的联系和差别"><span>📌 它们的联系和差别</span></a></h2><table><thead><tr><th style="text-align:left;">项目</th><th style="text-align:left;">传统模型（回归、决策树等）</th><th style="text-align:left;">神经网络</th></tr></thead><tbody><tr><td style="text-align:left;">属于机器学习</td><td style="text-align:left;">✅</td><td style="text-align:left;">✅</td></tr><tr><td style="text-align:left;">能处理非线性关系</td><td style="text-align:left;">部分可以（决策树、SVM等）</td><td style="text-align:left;">非常擅长</td></tr><tr><td style="text-align:left;">需要手动特征工程</td><td style="text-align:left;">多数需要</td><td style="text-align:left;">可以自动学习复杂特征组合</td></tr><tr><td style="text-align:left;">可解释性</td><td style="text-align:left;">比较强（决策路径、权重）</td><td style="text-align:left;">差（黑盒）</td></tr><tr><td style="text-align:left;">计算复杂度</td><td style="text-align:left;">低</td><td style="text-align:left;">高</td></tr><tr><td style="text-align:left;">适用数据规模</td><td style="text-align:left;">小~中型</td><td style="text-align:left;">中~超大型</td></tr></tbody></table><hr><h2 id="📌-总结一句" tabindex="-1"><a class="header-anchor" href="#📌-总结一句"><span>📌 总结一句：</span></a></h2><p>👉 <strong>神经网络是机器学习模型中的一种，属于复杂的非线性模型，和其他机器学习方法（线性回归、逻辑回归、决策树等）处于同一个大家族里，但能力、特点和适用场景不同</strong>。</p><hr><p>Q:抽象一下传统模型的模型公式和神经网络的模型公式。就像伪代码，不一定准确，主要表达含义</p><p>很好！咱们来抽象一下<strong>传统机器学习模型</strong>和<strong>神经网络模型</strong>的公式表达方式，写成伪公式（表达结构和含义，不追求数学严谨性）。</p><hr><h2 id="📌-传统机器学习模型公式抽象" tabindex="-1"><a class="header-anchor" href="#📌-传统机器学习模型公式抽象"><span>📌 传统机器学习模型公式抽象</span></a></h2><h3 id="📖-线性回归-逻辑回归" tabindex="-1"><a class="header-anchor" href="#📖-线性回归-逻辑回归"><span>📖 线性回归 / 逻辑回归</span></a></h3><div class="language-text line-numbers-mode" data-highlighter="shiki" data-ext="text" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-text"><span class="line"><span>y = w1 * x1 + w2 * x2 + ... + wn * xn + b</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>如果是逻辑回归，多加一个 Sigmoid：</p><div class="language-text line-numbers-mode" data-highlighter="shiki" data-ext="text" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-text"><span class="line"><span>y = Sigmoid(w1 * x1 + w2 * x2 + ... + wn * xn + b)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>核心特性：</p><ul><li>特征值 × 权重 → 相加 → (可选激活) → 输出</li><li>公式是线性的，或者加上简单的非线性激活（如sigmoid）</li></ul><hr><h3 id="📖-决策树" tabindex="-1"><a class="header-anchor" href="#📖-决策树"><span>📖 决策树</span></a></h3><div class="language-text line-numbers-mode" data-highlighter="shiki" data-ext="text" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-text"><span class="line"><span>if x1 &lt; a:</span></span>\n<span class="line"><span>    if x2 &lt; b:</span></span>\n<span class="line"><span>        y = c1</span></span>\n<span class="line"><span>    else:</span></span>\n<span class="line"><span>        y = c2</span></span>\n<span class="line"><span>else:</span></span>\n<span class="line"><span>    if x3 &lt; d:</span></span>\n<span class="line"><span>        y = c3</span></span>\n<span class="line"><span>    else:</span></span>\n<span class="line"><span>        y = c4</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>核心特性：</p><ul><li>条件判断式，逐层分裂，最终叶子节点给结果</li><li>没有显式的加权求和</li></ul><hr><h2 id="📌-神经网络模型公式抽象" tabindex="-1"><a class="header-anchor" href="#📌-神经网络模型公式抽象"><span>📌 神经网络模型公式抽象</span></a></h2><h3 id="📖-单层感知器-最简单的神经网络" tabindex="-1"><a class="header-anchor" href="#📖-单层感知器-最简单的神经网络"><span>📖 单层感知器（最简单的神经网络）</span></a></h3><div class="language-text line-numbers-mode" data-highlighter="shiki" data-ext="text" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-text"><span class="line"><span>y = Activation(w1 * x1 + w2 * x2 + ... + wn * xn + b)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>和逻辑回归几乎一样，只是激活函数可以选 ReLU、Tanh、Sigmoid 等</p><hr><h3 id="📖-多层前馈神经网络-mlp" tabindex="-1"><a class="header-anchor" href="#📖-多层前馈神经网络-mlp"><span>📖 多层前馈神经网络（MLP）</span></a></h3><div class="language-text line-numbers-mode" data-highlighter="shiki" data-ext="text" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-text"><span class="line"><span># 输入层 → 隐藏层 → 输出层</span></span>\n<span class="line"><span></span></span>\n<span class="line"><span># 第一层：</span></span>\n<span class="line"><span>h1 = Activation(W1 * X + B1)</span></span>\n<span class="line"><span></span></span>\n<span class="line"><span># 第二层：</span></span>\n<span class="line"><span>h2 = Activation(W2 * h1 + B2)</span></span>\n<span class="line"><span></span></span>\n<span class="line"><span># ...</span></span>\n<span class="line"><span># 最后输出层：</span></span>\n<span class="line"><span>y = Activation(W_output * hN + B_output)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>或者简化写成：</p><div class="language-text line-numbers-mode" data-highlighter="shiki" data-ext="text" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-text"><span class="line"><span>for each layer:</span></span>\n<span class="line"><span>    h = Activation(W * input + B)</span></span>\n<span class="line"><span>    input = h</span></span>\n<span class="line"><span>output = h</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>核心特性：</p><ul><li>每层是<strong>线性变换 + 非线性激活</strong></li><li>层与层相连，形成复合函数</li><li>网络越深，能拟合越复杂的非线性关系</li></ul><hr><h2 id="📌-总结对比" tabindex="-1"><a class="header-anchor" href="#📌-总结对比"><span>📌 总结对比</span></a></h2><table><thead><tr><th style="text-align:left;">模型</th><th style="text-align:left;">结构公式</th><th style="text-align:left;">特点</th></tr></thead><tbody><tr><td style="text-align:left;">线性回归</td><td style="text-align:left;">y = Σ (wi * xi) + b</td><td style="text-align:left;">线性组合</td></tr><tr><td style="text-align:left;">逻辑回归</td><td style="text-align:left;">y = Sigmoid(Σ (wi * xi) + b)</td><td style="text-align:left;">线性 + Sigmoid</td></tr><tr><td style="text-align:left;">决策树</td><td style="text-align:left;">if 条件 then 值 else 值</td><td style="text-align:left;">条件分裂</td></tr><tr><td style="text-align:left;">神经网络</td><td style="text-align:left;">y = Activation(Wn * ... * Activation(W1 * X + b1) + bn)</td><td style="text-align:left;">多层线性+非线性嵌套</td></tr></tbody></table><hr><p>如果你愿意，我还能帮你画个流程图式结构表达出来，或者写个 Python 伪代码版。要的话直接说 👍</p>',58)]))}]]),l=JSON.parse('{"path":"/posts/AI/AI%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%E5%85%B3%E7%B3%BB.html","title":"神经网络和机器学习经典模型关系","lang":"zh-CN","frontmatter":{"date":"2025-07-11T00:00:00.000Z","tag":["AI"],"description":"神经网络和机器学习经典模型关系 Q:比如房价和距离模型问题，可以用线性回归模型。还有一些其他的逻辑回归，决策树等模型。这些机器学习模型和神经网络的关系是什么？ Runner2011: 神经网络和经典模型的关系： 每一层的神经元做的事都是一个经典模型(例如逻辑回归模型)做的事 优化：经典模型直接用梯度算法。NN用的反向传播(也会用到梯度算法)。 很好！我...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"神经网络和机器学习经典模型关系\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-07-11T00:00:00.000Z\\",\\"dateModified\\":\\"2025-07-12T10:51:57.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Runner2011\\",\\"url\\":\\"https://runner2011.github.io\\"}]}"],["meta",{"property":"og:url","content":"https://runner2011.github.io/posts/AI/AI%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%92%8C%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BB%8F%E5%85%B8%E6%A8%A1%E5%9E%8B%E5%85%B3%E7%B3%BB.html"}],["meta",{"property":"og:site_name","content":"Runner2011 blog"}],["meta",{"property":"og:title","content":"神经网络和机器学习经典模型关系"}],["meta",{"property":"og:description","content":"神经网络和机器学习经典模型关系 Q:比如房价和距离模型问题，可以用线性回归模型。还有一些其他的逻辑回归，决策树等模型。这些机器学习模型和神经网络的关系是什么？ Runner2011: 神经网络和经典模型的关系： 每一层的神经元做的事都是一个经典模型(例如逻辑回归模型)做的事 优化：经典模型直接用梯度算法。NN用的反向传播(也会用到梯度算法)。 很好！我..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-07-12T10:51:57.000Z"}],["meta",{"property":"article:tag","content":"AI"}],["meta",{"property":"article:published_time","content":"2025-07-11T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-07-12T10:51:57.000Z"}]]},"git":{"createdTime":1752317517000,"updatedTime":1752317517000,"contributors":[{"name":"Runner2011","username":"Runner2011","email":"chenjfsea@gmail.com","commits":1,"url":"https://github.com/Runner2011"}]},"readingTime":{"minutes":4.63,"words":1389},"filePathRelative":"posts/AI/AI概念入门/神经网络和机器学习经典模型关系.md","excerpt":"\\n<p>Q:比如房价和距离模型问题，可以用线性回归模型。还有一些其他的逻辑回归，决策树等模型。这些机器学习模型和神经网络的关系是什么？</p>\\n<p>Runner2011:<br>\\n神经网络和经典模型的关系：</p>\\n<ul>\\n<li>每一层的神经元做的事都是一个经典模型(例如逻辑回归模型)做的事</li>\\n<li>优化：经典模型直接用梯度算法。NN用的反向传播(也会用到梯度算法)。</li>\\n</ul>\\n<hr>\\n<p>很好！我来给你捋清楚<strong>机器学习模型（如线性回归、逻辑回归、决策树等）和神经网络</strong>之间的关系和区别，咱们按体系讲清楚：</p>\\n<hr>\\n<h2>📌 机器学习模型体系</h2>","autoDesc":true}')}}]);