"use strict";(self.webpackChunkrunn2011blog=self.webpackChunkrunn2011blog||[]).push([[2072],{628:(t,a)=>{a.A=(t,a)=>{const s=t.__vccOpts||t;for(const[t,n]of a)s[t]=n;return s}},9512:(t,a,s)=>{s.r(a),s.d(a,{comp:()=>d,data:()=>r});var n=s(8178);const e={},d=(0,s(628).A)(e,[["render",function(t,a){return(0,n.uX)(),(0,n.CE)("div",null,a[0]||(a[0]=[(0,n.Fv)('<h1 id="ai常见词汇-1-算力" tabindex="-1"><a class="header-anchor" href="#ai常见词汇-1-算力"><span>AI常见词汇 (1) - 算力</span></a></h1><h2 id="什么是算力" tabindex="-1"><a class="header-anchor" href="#什么是算力"><span>什么是算力</span></a></h2><p>“算力”是“计算能力”的简称，用来衡量计算系统（如CPU、GPU、TPU、AI芯片、超算集群等）处理任务的能力</p><h2 id="算力怎么表示" tabindex="-1"><a class="header-anchor" href="#算力怎么表示"><span>算力怎么表示</span></a></h2><p>🔢一般常见的算力单位</p><table><thead><tr><th>单位</th><th>全称</th><th>用途举例</th></tr></thead><tbody><tr><td>FLOPS</td><td>Floating Point Operations Per Second（每秒浮点运算次数）</td><td>通用计算能力衡量（科学计算、AI推理等）</td></tr><tr><td>TOPS</td><td>Tera Operations Per Second（每秒万亿次操作）</td><td>常用于AI芯片（整数运算、低精度推理）</td></tr><tr><td>MIPS/GIPS</td><td>Million/Billion Instructions Per Second（每秒百万/十亿条指令）</td><td>常用于传统嵌入式/CPU性能测量</td></tr><tr><td>Hash/s</td><td>Hashes per second（每秒哈希数）</td><td>用于区块链/挖矿领域</td></tr><tr><td>TFLOPS/EFLOPS</td><td>Tera/Exa FLOPS（每秒万亿/百亿亿次浮点运算）</td><td>高性能计算（如超算、AI训练）衡量单位</td></tr></tbody></table><p>假设一个GPU有：<br> 单精度浮点性能为 10 TFLOPS，这意味着它每秒可以执行 10 × 10¹² 次浮点运算。</p><p><strong>设备算力对比表</strong></p><table><thead><tr><th>项目</th><th>Apple A17 Pro</th><th>Jetson Orin Nano</th><th>Switch 2（推测）</th><th>树莓派 5</th></tr></thead><tbody><tr><td>架构</td><td>ARM + NPU + GPU</td><td>ARM + Ampere GPU</td><td>ARM + Ampere GPU</td><td>ARM Cortex-A76 (4核)</td></tr><tr><td>GPU</td><td>自研 5核 + 光追</td><td>1024 CUDA (Ampere)</td><td>~1280 CUDA (Ampere)</td><td>VideoCore VII</td></tr><tr><td>AI算力</td><td>35 TOPS (NPU, INT8)</td><td>40 TOPS (INT8)</td><td>&lt;10 TOPS（估计）</td><td>❌ 无NPU（需靠CPU）</td></tr><tr><td>GPU浮点性能</td><td>23 TFLOPS（估算）</td><td>~1.5 TFLOPS FP32</td><td>2~4 TFLOPS FP32</td><td>~100 GFLOPS（估算）</td></tr><tr><td>训练能力</td><td>❌ 基本无</td><td>⚠️ 仅极简小模型</td><td>❌ 游戏平台，非训练用</td><td>❌ CPU训练极慢</td></tr><tr><td>内存</td><td>8 GB</td><td>LPDDR5 8~16 GB</td><td>LPDDR5 8~12 GB</td><td>LPDDR5X 4~8 GB</td></tr><tr><td>功耗</td><td>~3W</td><td>7~15W</td><td>10~18W</td><td>3~5W</td></tr><tr><td>定位</td><td>移动端</td><td>AI + 渲染</td><td>边缘 AI 推理</td><td>游戏主机</td></tr></tbody></table><h2 id="训练-vs-推理" tabindex="-1"><a class="header-anchor" href="#训练-vs-推理"><span>训练 vs 推理</span></a></h2><p>🧠 目的不同</p><table><thead><tr><th>项目</th><th>训练（Training）</th><th>推理（Inference）</th></tr></thead><tbody><tr><td>目标</td><td>学习模型参数（权重）</td><td>使用学到的模型进行实际应用</td></tr><tr><td>功能</td><td>通过大量数据优化模型，使其具备预测能力</td><td>用训练好的模型对新数据进行预测、分类等</td></tr></tbody></table><p>💻 计算资源需求</p><table><thead><tr><th>项目</th><th>训练</th><th>推理</th></tr></thead><tbody><tr><td>运算类型</td><td>以 FP32、FP16 为主（高精度浮点数）</td><td>多用 INT8、FP16（可量化、低功耗）</td></tr><tr><td>算力需求</td><td>极高：需大量矩阵乘法、梯度下降等复杂操作</td><td>较低：只需前向传播</td></tr><tr><td>硬件设备</td><td>GPU/TPU/NPU/高性能服务器</td><td>手机、PC、嵌入式设备（也可在云上）</td></tr><tr><td>时间成本</td><td>长（可几小时到几周）</td><td>快速（通常毫秒级）</td></tr></tbody></table><p>📉 表格总结（简洁版）</p><table><thead><tr><th>对比项</th><th>训练（Training）</th><th>推理（Inference）</th></tr></thead><tbody><tr><td>是否更新权重</td><td>✅ 是</td><td>❌ 否</td></tr><tr><td>是否反向传播</td><td>✅ 有</td><td>❌ 无</td></tr><tr><td>数据类型</td><td>有标签大量数据</td><td>无标签新数据</td></tr><tr><td>运算精度</td><td>高（FP32、FP16）</td><td>可低（INT8、FP16）</td></tr><tr><td>硬件</td><td>GPU/TPU</td><td>手机/边缘/云端服务器</td></tr><tr><td>速度</td><td>慢（小时~周）</td><td>快（毫秒~秒）</td></tr><tr><td>应用场景</td><td>模型构建</td><td>模型使用</td></tr></tbody></table><h3 id="为什么训练参数精度以f32-f16为主-推理以int8-f16-int4为主" tabindex="-1"><a class="header-anchor" href="#为什么训练参数精度以f32-f16为主-推理以int8-f16-int4为主"><span>为什么训练参数精度以F32,F16为主，推理以INT8,F16,INT4为主</span></a></h3><p>训练梯度微小，需精度，支持反向传播。推理速度优先，可量化，训练后部署。</p><h2 id="什么是ai算力" tabindex="-1"><a class="header-anchor" href="#什么是ai算力"><span>什么是AI算力</span></a></h2><p>AI算力主要指处理AI计算的能力，一般通过&quot;人工智能加速模块&quot;实现<br> P.S. AI计算，有大量的矩阵乘加法(MAC)操作,比如CNN卷积操作。 所以提供AI算力要针对性的设计矩阵成加法计算单元。</p><h2 id="npu是什么" tabindex="-1"><a class="header-anchor" href="#npu是什么"><span>NPU是什么</span></a></h2><ul><li>NPU = 神经网络处理器，专为加速深度学习模型（尤其是推理）设计。</li><li>通常支持 INT8/INT4/FP16 等低精度运算，每秒可处理数十亿~万亿次操作（TOPS）。</li><li>手机的 Apple Neural Engine、华为昇腾 NPU、NVIDIA Tensor Core、Google Edge TPU 都属于此类</li></ul><h3 id="" tabindex="-1"><a class="header-anchor" href="#"><span></span></a></h3><p>📌 一、为什么通用 CPU/GPU 不适合高效 AI 推理</p><h4 id="💡-ai推理的核心特征" tabindex="-1"><a class="header-anchor" href="#💡-ai推理的核心特征"><span>💡 AI推理的核心特征：</span></a></h4><ul><li>主要是 <strong>矩阵乘法</strong>（如 <code>Y = W·X + B</code>）</li><li>大量的 <strong>乘加（MAC）操作</strong>，而不是通用逻辑运算</li><li>并不要求很高的浮点精度（不像科学计算）</li></ul><h4 id="🚫-cpu-问题" tabindex="-1"><a class="header-anchor" href="#🚫-cpu-问题"><span>🚫 CPU 问题：</span></a></h4><ul><li>CPU 是通用控制器 + 大 cache，<strong>适合分支跳转、系统调用等</strong>控制逻辑</li><li>缺乏大量并行计算单元</li><li>SIMD 指令支持 INT8/FP16 也有限</li></ul><h4 id="⚠️-gpu-虽好-但仍有问题" tabindex="-1"><a class="header-anchor" href="#⚠️-gpu-虽好-但仍有问题"><span>⚠️ GPU 虽好，但仍有问题：</span></a></h4><ul><li><p>虽然支持浮点并行，但：</p><ul><li>传统 GPU 优化的是 <strong>图形流水线</strong>，不是矩阵乘法</li><li>对 AI 推理的低精度（INT8/INT4）支持弱（过去时代）</li><li>功耗高，不适合边缘设备</li></ul></li></ul><hr><h3 id="✅-二、npu-是怎么为-ai-定制的-从架构出发" tabindex="-1"><a class="header-anchor" href="#✅-二、npu-是怎么为-ai-定制的-从架构出发"><span>✅ 二、NPU 是怎么为 AI 定制的？（从架构出发）</span></a></h3><p>NPU（Neural Processing Unit）的核心设计思想是：</p><blockquote><p>“将 AI 中的大量矩阵乘加（MAC）操作，用专门的硬件阵列并行处理，并支持低精度数据格式以提升吞吐和节能。”</p></blockquote><hr><h3 id="🧱-三、npu-的硬件结构层次图" tabindex="-1"><a class="header-anchor" href="#🧱-三、npu-的硬件结构层次图"><span>🧱 三、NPU 的硬件结构层次图</span></a></h3><p>我们来看看典型的 NPU 结构：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>          ┌──────────────────────────────┐</span></span>\n<span class="line"><span>          │         控制模块（FSM）       │</span></span>\n<span class="line"><span>          └──────────────────────────────┘</span></span>\n<span class="line"><span>                     │</span></span>\n<span class="line"><span>          ┌──────────────────────────────┐</span></span>\n<span class="line"><span>          │  调度 / 指令解码 / 流水调度   │</span></span>\n<span class="line"><span>          └──────────────────────────────┘</span></span>\n<span class="line"><span>                     │</span></span>\n<span class="line"><span>          ┌──────────────────────────────┐</span></span>\n<span class="line"><span>          │   Compute Core（MAC阵列）    │ &lt;─ 支持 INT8/INT4/FP16</span></span>\n<span class="line"><span>          └──────────────────────────────┘</span></span>\n<span class="line"><span>               │      │       │</span></span>\n<span class="line"><span>     ┌─────────┘      │       └────────────┐</span></span>\n<span class="line"><span>     ▼                ▼                    ▼</span></span>\n<span class="line"><span>Local SRAM      Weight Buffer        Activation Buffer</span></span>\n<span class="line"><span>(支持片上访问)  (卷积核)            (输入特征图)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><hr><h3 id="⚙️-四、mac-单元-核心计算引擎" tabindex="-1"><a class="header-anchor" href="#⚙️-四、mac-单元-核心计算引擎"><span>⚙️ 四、MAC 单元：核心计算引擎</span></a></h3><h4 id="✅-mac-multiply-accumulate" tabindex="-1"><a class="header-anchor" href="#✅-mac-multiply-accumulate"><span>✅ MAC = Multiply + Accumulate</span></a></h4><p>例如，INT8 推理时的卷积操作：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">O</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">t</mi></mrow><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mo>=</mo><munder><mo>∑</mo><mi>j</mi></munder><mi>W</mi><mo stretchy="false">[</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">]</mo><mo>⋅</mo><mi>X</mi><mo stretchy="false">[</mo><mi>j</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex"> \\mathrm{Output}[i] = \\sum_j W[i,j] \\cdot X[j] </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathrm">Output</span></span><span class="mopen">[</span><span class="mord mathnormal">i</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4638em;vertical-align:-1.4138em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4138em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mopen">[</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mclose">]</span></span></span></span></span></p><p>NPU 内部会集成成 <strong>数千~上万个 MAC 单元</strong>，组成 <strong>矩阵计算阵列（如 systolic array）</strong>。</p><p>🔧 典型布局举例：</p><ul><li>64×64 MAC 阵列 ⇒ 一次可并行完成 <code>64×64</code> 的矩阵乘法</li><li>每个 MAC 支持 INT8 乘法 + INT32 累加（称为 INT8 → INT32）</li></ul><hr><h3 id="🧮-五、为什么使用-int8、int4、fp16-而不是-fp32" tabindex="-1"><a class="header-anchor" href="#🧮-五、为什么使用-int8、int4、fp16-而不是-fp32"><span>🧮 五、为什么使用 INT8、INT4、FP16 而不是 FP32？</span></a></h3><table><thead><tr><th>数据格式</th><th>位数</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td><strong>FP32</strong></td><td>32</td><td>高精度，兼容性强</td><td>慢，功耗高，面积大</td></tr><tr><td><strong>FP16/BF16</strong></td><td>16</td><td>更省资源，适合训练</td><td>精度可能不足</td></tr><tr><td><strong>INT8</strong></td><td>8</td><td>非常高效，推理常用</td><td>精度要靠量化保证</td></tr><tr><td><strong>INT4</strong></td><td>4</td><td>极限压缩，需专门算法</td><td>精度损失较大</td></tr></tbody></table><p>📉 精度 vs 资源消耗示意：</p><div class="language- line-numbers-mode" data-highlighter="shiki" data-ext="" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code class="language-"><span class="line"><span>FP32  &gt; FP16  &gt; INT8  &gt; INT4</span></span>\n<span class="line"><span>  ↑         ↑        ↑        ↑</span></span>\n<span class="line"><span>最耗资源                    最省资源</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><hr><h3 id="📏-六、算力单位如何体现这个低精度特性" tabindex="-1"><a class="header-anchor" href="#📏-六、算力单位如何体现这个低精度特性"><span>📏 六、算力单位如何体现这个低精度特性？</span></a></h3><h4 id="单位回顾" tabindex="-1"><a class="header-anchor" href="#单位回顾"><span>单位回顾：</span></a></h4><ul><li><strong>1 FLOPS</strong>：一次 FP32 运算（加、乘）</li><li><strong>1 TOPS</strong>：一次整数操作（通常是 INT8 MAC）</li><li>所以：</li></ul><blockquote><p><strong>同样硬件面积下，INT8 运算的并行度比 FP32 高 4 倍以上</strong><br> INT4 并行度更高 → 可达到更高 TOPS</p></blockquote><hr><h3 id="🔁-七、npu的-低精度-高并行-如何发挥巨大优势" tabindex="-1"><a class="header-anchor" href="#🔁-七、npu的-低精度-高并行-如何发挥巨大优势"><span>🔁 七、NPU的“低精度+高并行”如何发挥巨大优势？</span></a></h3><p>举个例子：</p><ul><li><p>面积单位下，你可以放：</p><ul><li>1 个 FP32 乘法器</li><li>或 4 个 FP16</li><li>或 16 个 INT8</li><li>或 32 个 INT4</li></ul></li></ul><p>所以：</p><blockquote><p>同一片面积和功耗预算下，NPU 可以用低精度堆出超高吞吐量（TOPS），远超 CPU/GPU</p></blockquote><p>这就是为什么：</p><ul><li>手机 NPU（A17 Pro）能达 <strong>35 TOPS</strong></li><li>Jetson Orin Nano 能达 <strong>40 TOPS</strong></li><li>而同面积的 CPU/GPU 无法接近这种密度</li></ul><hr><h3 id="🔄-八、为什么训练更偏好-fp32-fp16-而推理偏好-int8-int4" tabindex="-1"><a class="header-anchor" href="#🔄-八、为什么训练更偏好-fp32-fp16-而推理偏好-int8-int4"><span>🔄 八、为什么训练更偏好 FP32/FP16，而推理偏好 INT8/INT4？</span></a></h3><table><thead><tr><th>阶段</th><th>偏好精度</th><th>原因</th></tr></thead><tbody><tr><td><strong>训练</strong></td><td>FP32 → FP16/BF16</td><td>梯度微小，需精度，支持反向传播</td></tr><tr><td><strong>推理</strong></td><td>INT8 / INT4</td><td>速度优先，可量化，训练后部署</td></tr></tbody></table><hr><h3 id="✅-九、总结一句话" tabindex="-1"><a class="header-anchor" href="#✅-九、总结一句话"><span>✅ 九、总结一句话：</span></a></h3><blockquote><p><strong>NPU 是为神经网络中的大规模矩阵乘加而生的专用加速器，采用低精度（INT8/INT4/FP16）+ 大规模 MAC 并行阵列，实现高吞吐、低功耗的 AI 算力。</strong></p></blockquote>',70)]))}]]),r=JSON.parse('{"path":"/posts/AI/AI%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/AI%E5%B8%B8%E8%A7%81%E8%AF%8D%E6%B1%871.html","title":"AI常见词汇 (1) - 算力","lang":"zh-CN","frontmatter":{"tag":["AI_GEN"],"description":"AI常见词汇 (1) - 算力 什么是算力 “算力”是“计算能力”的简称，用来衡量计算系统（如CPU、GPU、TPU、AI芯片、超算集群等）处理任务的能力 算力怎么表示 🔢一般常见的算力单位 假设一个GPU有： 单精度浮点性能为 10 TFLOPS，这意味着它每秒可以执行 10 × 10¹² 次浮点运算。 设备算力对比表 训练 vs 推理 🧠 目的...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"AI常见词汇 (1) - 算力\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-07-25T06:33:37.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Runner2011\\",\\"url\\":\\"https://runner2011.github.io\\"}]}"],["meta",{"property":"og:url","content":"https://runner2011.github.io/posts/AI/AI%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/AI%E5%B8%B8%E8%A7%81%E8%AF%8D%E6%B1%871.html"}],["meta",{"property":"og:site_name","content":"Runner2011 blog"}],["meta",{"property":"og:title","content":"AI常见词汇 (1) - 算力"}],["meta",{"property":"og:description","content":"AI常见词汇 (1) - 算力 什么是算力 “算力”是“计算能力”的简称，用来衡量计算系统（如CPU、GPU、TPU、AI芯片、超算集群等）处理任务的能力 算力怎么表示 🔢一般常见的算力单位 假设一个GPU有： 单精度浮点性能为 10 TFLOPS，这意味着它每秒可以执行 10 × 10¹² 次浮点运算。 设备算力对比表 训练 vs 推理 🧠 目的..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-07-25T06:33:37.000Z"}],["meta",{"property":"article:tag","content":"AI_GEN"}],["meta",{"property":"article:modified_time","content":"2025-07-25T06:33:37.000Z"}]]},"git":{"createdTime":1753425217000,"updatedTime":1753425217000,"contributors":[{"name":"Runner2011","username":"Runner2011","email":"chenjfsea@gmail.com","commits":1,"url":"https://github.com/Runner2011"}]},"readingTime":{"minutes":5.78,"words":1733},"filePathRelative":"posts/AI/AI概念入门/AI常见词汇1.md","excerpt":"\\n<h2>什么是算力</h2>\\n<p>“算力”是“计算能力”的简称，用来衡量计算系统（如CPU、GPU、TPU、AI芯片、超算集群等）处理任务的能力</p>\\n<h2>算力怎么表示</h2>\\n<p>🔢一般常见的算力单位</p>\\n<table>\\n<thead>\\n<tr>\\n<th>单位</th>\\n<th>全称</th>\\n<th>用途举例</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>FLOPS</td>\\n<td>Floating Point Operations Per Second（每秒浮点运算次数）</td>\\n<td>通用计算能力衡量（科学计算、AI推理等）</td>\\n</tr>\\n<tr>\\n<td>TOPS</td>\\n<td>Tera Operations Per Second（每秒万亿次操作）</td>\\n<td>常用于AI芯片（整数运算、低精度推理）</td>\\n</tr>\\n<tr>\\n<td>MIPS/GIPS</td>\\n<td>Million/Billion Instructions Per Second（每秒百万/十亿条指令）</td>\\n<td>常用于传统嵌入式/CPU性能测量</td>\\n</tr>\\n<tr>\\n<td>Hash/s</td>\\n<td>Hashes per second（每秒哈希数）</td>\\n<td>用于区块链/挖矿领域</td>\\n</tr>\\n<tr>\\n<td>TFLOPS/EFLOPS</td>\\n<td>Tera/Exa FLOPS（每秒万亿/百亿亿次浮点运算）</td>\\n<td>高性能计算（如超算、AI训练）衡量单位</td>\\n</tr>\\n</tbody>\\n</table>","autoDesc":true}')}}]);