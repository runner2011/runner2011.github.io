"use strict";(self.webpackChunkrunn2011blog=self.webpackChunkrunn2011blog||[]).push([[5189],{2632:(r,n,t)=>{t.r(n),t.d(n,{comp:()=>e,data:()=>o});var i=t(641);const l={},e=(0,t(6262).A)(l,[["render",function(r,n){const t=(0,i.g2)("RouteLink");return(0,i.uX)(),(0,i.CE)("div",null,[n[3]||(n[3]=(0,i.Fv)('<h1 id="john-carmack-在-upper-bound-2025-上的演讲分享总结" tabindex="-1"><a class="header-anchor" href="#john-carmack-在-upper-bound-2025-上的演讲分享总结"><span>John Carmack 在 Upper Bound 2025 上的演讲分享总结</span></a></h1><p>以下是基于提供的文档，对约翰·卡马克（John Carmack）在上界（UpperBound）25会议上的演讲笔记进行详细的中文翻译总结，尽可能全面、清晰地呈现其职业背景、在Keen Technologies的研究工作、对强化学习（RL）和人工智能（AI）的看法，以及当前基于Atari游戏的研究项目。</p><hr><h3 id="_1-职业背景概述" tabindex="-1"><a class="header-anchor" href="#_1-职业背景概述"><span><strong>1. 职业背景概述</strong></span></a></h3><p>约翰·卡马克作为科技领域的传奇人物，分享了他的职业历程，为当前的人工智能和强化学习工作提供背景：</p><ul><li><p><strong>Id Software（1990年代）</strong>：作为创始人，卡马克开发了《指挥官基恩》（Commander Keen）、《德军总部3D》（Wolfenstein 3D）、《毁灭战士》（Doom）和《雷神之锤》（Quake）等经典游戏。他为《雷神之锤》推动GPU发展感到自豪，这间接促成了现代AI的兴起。DeepMind的DMLab环境也基于《雷神之锤竞技场》的优化版本。</p></li><li><p><strong>Armadillo Aerospace</strong>：与Id Software工作重叠，卡马克花了十年时间研究垂直起降（VTVL）火箭，展现了他对前沿工程的兴趣。</p></li><li><p><strong>Oculus / Meta</strong>：卡马克为现代虚拟现实（VR）奠定了技术基础，显著推动了如Oculus Quest等VR头显的发展。</p></li><li><p><strong>Keen Technologies</strong>：在Meta工作期间，OpenAI创始人试图招募他。尽管他并非AI专家，他深入研究后认为AI是“我能从事的最重要的事情”。这标志着他从系统工程转向研究的重大转变，他对此乐在其中。他与强化学习领域的“教父”理查德·萨顿（Richard Sutton）合作，萨顿的教科书他读了两遍。两人观点高度一致，但也有健康的意见分歧。目前团队有六名研究人员，背景涵盖学术界和工业界。</p></li></ul><hr><h3 id="_2-研究理念与ai方法" tabindex="-1"><a class="header-anchor" href="#_2-研究理念与ai方法"><span><strong>2. 研究理念与AI方法</strong></span></a></h3><p>卡马克的演讲反映了他对大型语言模型（LLM）的怀疑，以及对强化学习的关注，特别是受游戏背景启发的交互式经验学习。</p><ul><li><p><strong>对LLM的怀疑</strong>：卡马克承认LLM的惊人能力，但认为它们通过预训练“知道一切却不学任何东西”，类似于将所有数据“扔进搅拌机”。他主张基于交互式经验流的学习，如人类和动物那样，认为这更符合基础学习原理。</p></li><li><p><strong>对LLM取代RL的担忧</strong>：卡马克担心基于Transformer的基础模型可能最终超越传统RL，即使在未见过的环境中也能表现良好（即使效率低下）。这将符合“苦涩教训”（Bitter Lesson），即通用方法长期优于专用方法。</p></li><li><p><strong>游戏与虚拟环境</strong>：卡马克的游戏背景使他倾向于使用游戏作为RL的试验场。他提到DeepMind 2013年的Atari工作，模型仅通过像素输入学会玩数十款游戏，当时他认为这几乎不可能。然而，他指出局限性：</p><ul><li>RL模型需要2亿帧（相当于连续玩一个月）才能达到研究生一小时的学习水平。</li><li>某些游戏需要数十亿帧和专用架构才能接近人类表现。</li><li>在单一环境中实现持续、高效、终身学习仍未解决，RL系统远不如猫狗的适应能力。</li></ul></li><li><p><strong>初期失误</strong>：</p><ul><li><strong>低层次开发</strong>：卡马克起初用C++编写CUDA内核，逐步转向cuBLAS、cuDNN，最终采用PyTorch。他认识到高级工具在快速实验中的强大作用。</li><li><strong>延迟大规模实验</strong>：他在本地GPU和Jupyter笔记本上花费过多时间，最终转向通过Visual Studio Code使用远程主机，发现Lambda Labs表现良好。</li><li><strong>世嘉Master System</strong>：他曾尝试用世嘉Master System游戏作为比Atari更复杂的测试平台，但最终选择Atari，因其拥有广泛的研究历史，便于实验比较。</li><li><strong>视频学习</strong>：卡马克最初计划通过带有移动关注点的电视内容被动学习，以建立游戏学习的基础（如物体恒存性）。他后来推迟了这一想法，专注于Atari游戏的RL基础问题。</li></ul></li></ul><hr><h3 id="_3-当前研究-以atari为试验场" tabindex="-1"><a class="header-anchor" href="#_3-当前研究-以atari为试验场"><span><strong>3. 当前研究：以Atari为试验场</strong></span></a></h3><p>卡马克在Keen Technologies的团队利用Atari游戏探索RL挑战，强调其多样性、公正性及深厚的研究基础。</p><ul><li><p><strong>为何选择Atari？</strong></p><ul><li><strong>公正且多样</strong>：商业游戏未被研究者定制，玩法多样化能揭示RL的各种挑战（例如，为何某些技术在特定游戏中有效而在其他游戏中无效）。</li><li><strong>缺点</strong>： <ul><li>老旧硬件导致的精灵闪烁可能干扰学习。</li><li>Atari游戏的确定性行为易被利用，除非使用“粘性动作”（随机化动作延迟）。但Atari 100k基准测试多使用确定性动作，限制了真实性。</li></ul></li><li><strong>研究历史</strong>：Atari有大量前期研究，但旧RL框架运行困难。卡马克建议直接使用Arcade Learning Environment（ALE），避免通过Gymnasium封装。</li></ul></li><li><p><strong>Atari是否已“解决”？</strong></p><ul><li>最新模型如MEME、Muesli和BBF在数亿帧后，在多数Atari游戏中达到或超过人类最高得分，但仍有改进空间。卡马克更关注Atari 100k基准测试，强调数据效率（仅用10万帧学习）。</li><li>BBF和Efficient Zero在数据效率上显著改进，但结果因随机种子、不同游戏时长（如《Pitfall!》需20分钟）及评估方式而噪音较大。例如，<em>Breakout</em>的高分在粘性动作下存疑。</li></ul></li><li><p><strong>为何不选更复杂游戏？</strong></p><ul><li>卡马克避免如《精灵宝可梦》或《Minecraft》这样的复杂游戏，因为Atari的简单性仍存在未解研究问题。复杂游戏诱使研究者“作弊”，通过访问内部数据结构而非仅依赖像素输入。</li></ul></li></ul><hr><h3 id="_4-现实世界的rl挑战-物理atari设置" tabindex="-1"><a class="header-anchor" href="#_4-现实世界的rl挑战-物理atari设置"><span><strong>4. 现实世界的RL挑战：物理Atari设置</strong></span></a></h3><p>卡马克团队正在探索物理Atari 2600+设置，将RL从模拟器扩展到现实世界，以应对真实环境的复杂性。</p><ul><li><p><strong>设置</strong>：</p><ul><li>使用摄像头观察游戏屏幕，伺服电机控制摇杆，笔记本电脑实时运行RL代理。</li><li>测试游戏包括《吃豆人小姐》（MsPacman）、《蜈蚣》（Centipede）、《Qbert》、《保卫者》（Defender）、《Krull》、《Atlantis》、《UpnDown》和《战斗地带》（Battle Zone）。部分游戏可通过摇杆按钮重启，避免额外伺服器按压控制台重置开关。</li><li>摄像头分辨率为640x480用于学习，1920x1080有助于读取分数和April Tag屏幕校正。</li><li>USB I/O板驱动摇杆，但伺服电机引入了延迟和行为变异性，伺服器和摇杆故障是显著挑战。</li></ul></li><li><p><strong>主要挑战</strong>：</p><ul><li><p><strong>分数检测</strong>：为RL提供奖励的分数读取意外困难。多模态LLM可从截图读取分数，但专用分数读取器不可靠，尤其当分数隐藏（如《Yar’s Revenge》）或移动（如《Berzerk》）。《Qbert》的分数偶尔隐藏但可处理。</p></li><li><p><strong>摄像头问题</strong>：许多USB摄像头使用JPEG编码，损害图像质量并增加延迟，应避免。传统相机接口在整个图像扫描完成后才传输，导致顶部像素比底部像素“陈旧”16毫秒。Nvidia的GPU Direct for Video技术可通过逐行扫描重叠多层CNN处理，但扫描线问题（由于摄像头与视频扫描不同步）尚未成为显著问题。</p></li><li><p><strong>屏幕校正与April Tags</strong>：通过April Tags进行屏幕校正可识别游戏屏幕的关键部分，但若要让模型在摄像头位置变化时保留知识，需明确哪些屏幕部分对模型重要。光线条件会显著影响标签识别。</p></li><li><p><strong>摇杆动作</strong>：使用USB I/O板驱动摇杆，Khurram设计的Robotroller使用伺服电机控制真实摇杆，但增加了延迟和行为变异性。伺服器和摇杆的多次故障凸显了机器人可靠性的挑战。</p></li><li><p><strong>动作分解问题</strong>：例如，《Atlantis》需要区分“侧向移动”和“开火”动作。许多Atari RL模型未将上次动作纳入新观察中，但在这种情况下可能至关重要。卡马克提到“上、上、下、下、左、右、左、右、B、A”的Konami代码作为动作序列的例子，也反映了学习的局限性。</p></li><li><p><strong>分数与生命检测</strong>：分数检测是项目中最棘手的问题，因为RL算法依赖分数作为奖励。多模态LLM可从截图读取分数，但专用分数读取器在公开场合可能出错。某些游戏因分数不可靠被剔除：</p><ul><li>《Yar’s Revenge》大部分时间分数隐藏。</li><li>《Berzerk》分数位置移动且显示内容变化。</li><li>《Qbert》在玩家位于顶部时分数隐藏，但影响不大。</li></ul></li><li><p><strong>Atari开发箱</strong>：使用发光April Tags进行校正和分数读取，以提高可靠性。</p></li></ul></li><li><p><strong>开源计划</strong>：卡马克计划开源物理Atari工作，促进透明度和协作。</p></li></ul><hr><h3 id="_5-rl的挑战与洞见" tabindex="-1"><a class="header-anchor" href="#_5-rl的挑战与洞见"><span><strong>5. RL的挑战与洞见</strong></span></a></h3><p>卡马克讨论了Atari及其他RL应用的多个技术挑战，提供了深刻的见解：</p><ul><li><p><strong>速度与延迟</strong>：</p><ul><li><strong>速度</strong>：RL策略需快速评估以实时行动。传统算法在消费级硬件上可达到15帧/秒（动作重复4次），但新的高数据效率模型速度慢几个数量级。卡马克使用CUDA图、通过不同优先级的CUDA流重叠训练与环境/策略工作，并结合显式同步优化性能。</li><li><strong>延迟</strong>：现实世界的动作有显著延迟（例如，人类平均反应时间为160毫秒）。传统无模型RL算法（如DQN）能较好适应延迟，《Breakout》快速适应，《MsPacman》稍慢。但现代高性能算法假设动作立即生效，延迟会导致崩溃。添加延迟队列是临时解决办法，但未触及核心问题。卡马克建议在RL框架中加入几行代码实现动作“延迟线”，并认为这是现实环境带来的开放性挑战。</li></ul></li><li><p><strong>单一环境与并行环境</strong>：</p><ul><li>并行训练多个环境因样本多样性而更简单，但卡马克认为单一环境学习的有效性对AGI至关重要，因为生物智能在此表现优异。如果批次为1的训练失败，表明算法相对于生物智能存在缺陷，需解决。更大批次环境能加速学习，对AGI训练重要，但单一环境失败揭示了算法的根本问题。</li></ul></li><li><p><strong>稀疏奖励与好奇心</strong>：</p><ul><li>密集奖励（如分数增加）在现实任务中是例外，而非常态。人类玩游戏时很少关注分数变化，某些游戏（如《Yar’s Revenge》）分数仅在关卡间显示。卡马克建议通过仅在游戏结束或生命损失时提供奖励，将游戏变成“硬探索”任务。</li><li>人工好奇心驱动的内在奖励可能有所帮助，代理可能学会将屏幕上的分数作为内在奖励。</li><li><strong>硬探索游戏</strong>：如《Montezuma’s Revenge》和《Pitfall》长期是研究热点，虽有进展，但与人类效率相比仍有几个数量级的差距，研究方向是否正确尚不清楚。</li></ul></li><li><p><strong>序列多任务学习</strong>：</p><ul><li>序列学习（掌握一游戏，切换到其他游戏，再返回）比并行学习更难。当前RL系统在返回之前学习的游戏时表现接近随机，存在“灾难性遗忘”问题。人类能保留多年未练习的技能，而RL系统重新学习虽快于初学，但仍不可接受。</li><li>神经网络的近期偏见和权重更新影响所有输出，导致遗忘。卡马克探索的学习率调整、稀疏性和错误校正等方法。理查德和约瑟夫推崇的“流式RL”（避免回放缓冲区）理念，卡马克持怀疑态度，认为生物大脑的长期情景记忆与简单策略评估不同，难以仅靠流式RL学习多样化任务。</li><li>完美记忆（存储TB级观察数据）在技术上可行，但需解决如何有效利用。</li></ul></li><li><p><strong>迁移学习与可塑性</strong>：</p><ul><li>当前RL代理在学习新游戏时表现如同新手，即使已掌握多款游戏。这与人类利用先前知识的能力形成对比。OpenAI的《Gotta Learn Fast》挑战未解决此问题，获胜者仅在100万步内从头学习。</li><li>GATO显示负迁移学习，即并行学习其他游戏后，新游戏学习更难。保持网络的可塑性（学习新任务的能力）是一大挑战，可能与生物大脑老化（“老狗学不会新把戏”）有类似之处。</li><li>卡马克提议新基准测试，循环通过一组游戏，基于最后一轮的得分评估，平衡学习速度、避免遗忘和实现迁移学习。需使用粘性动作、无最小动作集，并在游戏切换时提供“截断”信号（区别于“终止”信号），以防算法误以为因切换而“死亡”。</li></ul></li><li><p><strong>探索与动作空间</strong>：</p><ul><li>从整个动作空间随机选择动作不可行。现代游戏控制器（如Xbox手柄）有数十亿种组合（忽略模拟输入），即使Atari的18动作空间与4动作空间相比影响较小，探索仍需优化。随机动作在《Breakout》或《Qbert》中可能导致生命损失，高分依赖极低的ε-贪婪值（例如0.99优于0.9）。</li><li>动作空间分解或学习动作集合可能改进探索。显式策略或带温度的softmax贪婪方法也值得尝试。</li></ul></li><li><p><strong>值表示</strong>：</p><ul><li>大范围标量值表示在生物学上不可信。卡马克提出极坐标值表示（带旋转环绕），关注动作的相对优劣，所有动作值通常在相近范围内。</li><li>传统DQN将奖励限制在-1/1以稳定学习并统一学习率，但卡马克发现即使不限制奖励，MSE损失也能处理如《Ms Pac-Man》（单点10分，奖励物数千分）与《Video Pinball》的大范围分数，尽管学习较慢。</li><li>分类值表示（简单高斯分布）优于完整分类操作，但忽略了值的序数性质，可能需如Earth Mover损失的改进方法。MuZero的二次值压缩不线性插值，且需范围超参数。</li></ul></li><li><p><strong>基于模型与无模型RL</strong>：</p><ul><li>显式任务ID或超参数斜坡是“作弊”。学习应隐式处理关卡变化与游戏切换。</li><li>离线RL（基于记忆学习）因缺乏持续现实测试易偏离轨道。大规模LLM依赖百万级批次和随机采样提供多样化梯度，顺序在线学习（如逐本书籍）会导致遗忘早期内容，即使IID采样批次为1，性能也显著低于预期。</li></ul></li><li><p><strong>函数逼近</strong>：</p><ul><li>神经网络同时处理新输入学习、相似输入泛化、随机过程样本平均和非平稳策略更新。权重更新影响所有输出，依赖泛化或中性噪声。初始化、激活函数、损失函数和优化器（如Adam）至关重要，但BBF重置网络显著提升性能，表明存在根本问题。</li><li>浮点计算和反向传播梯度可能并非最佳基础，背离标准实践令人担忧。辅助损失（如自预测表示）是迫使函数逼近器更好地服务于值和策略的方法。</li><li>在《Breakout》中，正确动作主要由代表球和拍子的少数像素决定，而非数千其他像素。高度相关的非平稳数据（分布漂移和概念漂移）使值估计噪声大，奖励可能在数百步后，中间的策略选择可能关键。</li></ul></li><li><p><strong>卷积神经网络（CNN）</strong>：</p><ul><li>CNN仍是图像处理的首选架构，但大型图像网络（如ConvNeXT）在RL中的表现不如简单步幅CNN。分类性能与在线RL性能不直接相关，探索差异可能有价值。</li><li>5x5内核是3x3内核的超集，但参数更多却常学得更差。权重衰减应减少噪声特征影响，但耗时长。共享权重在CNN中有效但生物学上不可信。无共享参数的测试在数百万步内表现更差，但长期可能更优。</li><li>分解为2个一维CNN在CIFAR-10上有帮助，但在RL中无效。各向同性CNN处理子像素偏移，但池化或步幅卷积破坏位置无关性。DenseNet（每通道访问之前所有信息）在各向同性CNN中有潜力。循环各向同性半密集CNN（无帧堆叠，每帧评估一次卷积）增加延迟但值得探索。</li></ul></li></ul><hr><h3 id="_6-更广泛的影响与未来方向" tabindex="-1"><a class="header-anchor" href="#_6-更广泛的影响与未来方向"><span><strong>6. 更广泛的影响与未来方向</strong></span></a></h3><ul><li><strong>现实与模拟</strong>：卡马克强调现实非回合制，与传统RL框架不同。物理Atari设置测试延迟、光线和机械可靠性等现实挑战，为实现具身AGI提供现实检验。</li><li><strong>元好奇心</strong>：卡马克设想代理基于好奇心选择游戏，模仿人类行为（例如，因无聊或挫败切换游戏）。奖励可基于提升人类标准化分数，需防止系统被操控（例如，故意低分后再高分）。可能需强制尝试其他游戏或设置最短游戏时间。</li><li><strong>可塑性与泛化</strong>：两者可能存在张力，泛化忽略细节，可塑性需识别新模式。近期和首因偏见复杂化学习，RL的策略/值函数不同于简单交互痕迹。CNN的归纳偏见有助于泛化，但理论基础薄弱。</li><li><strong>新基准测试</strong>：卡马克提议循环游戏集的基准测试，基于最后一轮得分评估，测试学习速度、避免遗忘和迁移学习。使用粘性动作、无最小动作集，并在切换游戏时提供“截断”信号。</li></ul><hr><h3 id="结论" tabindex="-1"><a class="header-anchor" href="#结论"><span><strong>结论</strong></span></a></h3><p>约翰·卡马克在上界25会议的演讲展示了他从系统工程到AI研究的转型，专注于以Atari游戏为试验场的强化学习。在Keen Technologies，他探索数据效率、灾难性遗忘、迁移学习和现实世界动态等RL核心挑战。通过物理Atari设置，他将模拟与现实结合，强调受生物启发的交互式经验学习。他对LLM持怀疑态度，认为Atari的简单性仍有未解研究问题，并计划开源物理Atari工作。他的技术洞见揭示了函数逼近、探索和值表示的复杂性，为AGI的RL发展提供了深刻思考。</p><hr>',29)),(0,i.Lk)("p",null,[n[1]||(n[1]=(0,i.eW)("原文：")),n[2]||(n[2]=(0,i.Lk)("br",null,null,-1)),(0,i.bF)(t,{to:"/Resources/UpperBound25.html"},{default:(0,i.k6)((()=>n[0]||(n[0]=[(0,i.eW)("演讲稿")]))),_:1,__:[0]})])])}]]),o=JSON.parse('{"path":"/posts/AI/MISC/John%20Carmack%20(Keen%20Technologies)_%20Research%20Directions%20at%20Upper%20Bound%202025.html","title":"John Carmack 在 Upper Bound 2025 上的演讲分享总结","lang":"zh-CN","frontmatter":{"tag":["AI_GEN"],"description":"John Carmack 在 Upper Bound 2025 上的演讲分享总结 以下是基于提供的文档，对约翰·卡马克（John Carmack）在上界（UpperBound）25会议上的演讲笔记进行详细的中文翻译总结，尽可能全面、清晰地呈现其职业背景、在Keen Technologies的研究工作、对强化学习（RL）和人工智能（AI）的看法，以及当前...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"John Carmack 在 Upper Bound 2025 上的演讲分享总结\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-07-05T13:29:15.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Runner2011\\",\\"url\\":\\"https://runner2011.github.io\\"}]}"],["meta",{"property":"og:url","content":"https://runner2011.github.io/posts/AI/MISC/John%20Carmack%20(Keen%20Technologies)_%20Research%20Directions%20at%20Upper%20Bound%202025.html"}],["meta",{"property":"og:site_name","content":"Runner2011 blog"}],["meta",{"property":"og:title","content":"John Carmack 在 Upper Bound 2025 上的演讲分享总结"}],["meta",{"property":"og:description","content":"John Carmack 在 Upper Bound 2025 上的演讲分享总结 以下是基于提供的文档，对约翰·卡马克（John Carmack）在上界（UpperBound）25会议上的演讲笔记进行详细的中文翻译总结，尽可能全面、清晰地呈现其职业背景、在Keen Technologies的研究工作、对强化学习（RL）和人工智能（AI）的看法，以及当前..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-07-05T13:29:15.000Z"}],["meta",{"property":"article:tag","content":"AI_GEN"}],["meta",{"property":"article:modified_time","content":"2025-07-05T13:29:15.000Z"}]]},"git":{"createdTime":1751722155000,"updatedTime":1751722155000,"contributors":[{"name":"Runner2011","username":"Runner2011","email":"chenjfsea@gmail.com","commits":1,"url":"https://github.com/Runner2011"}]},"readingTime":{"minutes":15.9,"words":4770},"filePathRelative":"posts/AI/MISC/John Carmack (Keen Technologies)_ Research Directions at Upper Bound 2025.md","excerpt":"\\n<p>以下是基于提供的文档，对约翰·卡马克（John Carmack）在上界（UpperBound）25会议上的演讲笔记进行详细的中文翻译总结，尽可能全面、清晰地呈现其职业背景、在Keen Technologies的研究工作、对强化学习（RL）和人工智能（AI）的看法，以及当前基于Atari游戏的研究项目。</p>\\n<hr>\\n<h3><strong>1. 职业背景概述</strong></h3>\\n<p>约翰·卡马克作为科技领域的传奇人物，分享了他的职业历程，为当前的人工智能和强化学习工作提供背景：</p>\\n<ul>\\n<li>\\n<p><strong>Id Software（1990年代）</strong>：作为创始人，卡马克开发了《指挥官基恩》（Commander Keen）、《德军总部3D》（Wolfenstein 3D）、《毁灭战士》（Doom）和《雷神之锤》（Quake）等经典游戏。他为《雷神之锤》推动GPU发展感到自豪，这间接促成了现代AI的兴起。DeepMind的DMLab环境也基于《雷神之锤竞技场》的优化版本。</p>\\n</li>\\n<li>\\n<p><strong>Armadillo Aerospace</strong>：与Id Software工作重叠，卡马克花了十年时间研究垂直起降（VTVL）火箭，展现了他对前沿工程的兴趣。</p>\\n</li>\\n<li>\\n<p><strong>Oculus / Meta</strong>：卡马克为现代虚拟现实（VR）奠定了技术基础，显著推动了如Oculus Quest等VR头显的发展。</p>\\n</li>\\n<li>\\n<p><strong>Keen Technologies</strong>：在Meta工作期间，OpenAI创始人试图招募他。尽管他并非AI专家，他深入研究后认为AI是“我能从事的最重要的事情”。这标志着他从系统工程转向研究的重大转变，他对此乐在其中。他与强化学习领域的“教父”理查德·萨顿（Richard Sutton）合作，萨顿的教科书他读了两遍。两人观点高度一致，但也有健康的意见分歧。目前团队有六名研究人员，背景涵盖学术界和工业界。</p>\\n</li>\\n</ul>","autoDesc":true}')},6262:(r,n)=>{n.A=(r,n)=>{const t=r.__vccOpts||r;for(const[r,i]of n)t[r]=i;return t}}}]);