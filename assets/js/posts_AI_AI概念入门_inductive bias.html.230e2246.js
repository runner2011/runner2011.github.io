"use strict";(self.webpackChunkrunn2011blog=self.webpackChunkrunn2011blog||[]).push([[1914],{628:(s,a)=>{a.A=(s,a)=>{const t=s.__vccOpts||s;for(const[s,n]of a)t[s]=n;return t}},8762:(s,a,t)=>{t.r(a),t.d(a,{comp:()=>e,data:()=>p});var n=t(8178);const i={},e=(0,t(628).A)(i,[["render",function(s,a){return(0,n.uX)(),(0,n.CE)("div",null,a[0]||(a[0]=[(0,n.Fv)('<h1 id="什么是inductive-bias" tabindex="-1"><a class="header-anchor" href="#什么是inductive-bias"><span>什么是inductive bias</span></a></h1><p>在 AI / 机器学习语境中，**inductive bias（归纳偏置）**是指：</p><blockquote><p>当训练数据不足以唯一确定一个函数时，模型为了能够做出泛化预测而“额外假设”的那部分结构性偏好。</p></blockquote><p>更形式化一点：</p><ul><li>设训练数据为 ( D = {(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i, y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>)} )</li><li>存在多个函数 ( f ) 都可以完美拟合 ( D )</li><li>模型最终选择其中某一个</li><li>这个“选择机制”背后的偏好，就是 inductive bias</li></ul><p>如果没有 inductive bias，模型根本无法泛化——这正是 <strong>No Free Lunch Theorem</strong> 的核心结论。</p><hr><h3 id="为什么一定需要-inductive-bias" tabindex="-1"><a class="header-anchor" href="#为什么一定需要-inductive-bias"><span>为什么一定需要 inductive bias？</span></a></h3><p>考虑一个极端例子：</p><p>你只见过：</p><ul><li>黑猫 → 哺乳动物</li><li>白猫 → 哺乳动物</li></ul><p>现在出现一只灰猫。</p><p>可能的函数空间里存在两种函数：</p><ol><li>所有猫都是哺乳动物</li><li>只有黑猫和白猫是哺乳动物，灰猫不是</li></ol><p>数据并没有排除第二种可能。</p><p>模型必须“假设”某种结构，比如：</p><ul><li>类别在特征空间上是连续的</li><li>相似输入产生相似输出</li><li>决策边界是平滑的</li></ul><p>这就是 inductive bias。</p><hr><h3 id="从数学角度理解" tabindex="-1"><a class="header-anchor" href="#从数学角度理解"><span>从数学角度理解</span></a></h3><p>机器学习的泛化能力来自：</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Generalization</mtext><mo>=</mo><mtext>Data</mtext><mo>+</mo><mtext>Inductive Bias</mtext></mrow><annotation encoding="application/x-tex"> \\text{Generalization} = \\text{Data} + \\text{Inductive Bias} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">Generalization</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord text"><span class="mord">Data</span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord text"><span class="mord">Inductive Bias</span></span></span></span></span></p><p>常见形式包括：</p><ul><li>限制假设空间（Hypothesis space restriction）</li><li>正则化（L1/L2）</li><li>模型结构设计</li><li>优化策略</li><li>先验分布（Bayesian prior）</li></ul><hr><h3 id="常见模型的-inductive-bias-示例" tabindex="-1"><a class="header-anchor" href="#常见模型的-inductive-bias-示例"><span>常见模型的 inductive bias 示例</span></a></h3><h4 id="_1️⃣-线性回归" tabindex="-1"><a class="header-anchor" href="#_1️⃣-线性回归"><span>1️⃣ 线性回归</span></a></h4><p>Bias：</p><ul><li>函数是线性的</li><li>参数较小（若有L2正则）</li></ul><p>数学形式：<br><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>w</mi><mi>T</mi></msup><mi>x</mi></mrow><annotation encoding="application/x-tex"> f(x) = w^T x </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathnormal">x</span></span></span></span></p><p>它隐含假设真实关系是线性或接近线性。</p><hr><h4 id="_2️⃣-卷积神经网络-cnn" tabindex="-1"><a class="header-anchor" href="#_2️⃣-卷积神经网络-cnn"><span>2️⃣ 卷积神经网络（CNN）</span></a></h4><p>Bias：</p><ul><li>局部性（local connectivity）</li><li>权重共享（translation equivariance）</li><li>空间结构重要</li></ul><p>这使得 CNN 在图像任务上数据效率极高。</p><hr><h4 id="_3️⃣-transformer" tabindex="-1"><a class="header-anchor" href="#_3️⃣-transformer"><span>3️⃣ Transformer</span></a></h4><p>Bias：</p><ul><li>序列结构</li><li>注意力机制假设任意位置之间可能有关联</li><li>位置编码假设顺序重要</li></ul><p>相比 CNN，它的 inductive bias 更弱，因此通常需要更大数据。</p><hr><h4 id="_4️⃣-knn" tabindex="-1"><a class="header-anchor" href="#_4️⃣-knn"><span>4️⃣ KNN</span></a></h4><p>Bias：</p><ul><li>邻近点具有相似标签（局部平滑性）</li></ul><hr><h3 id="强-inductive-bias-vs-弱-inductive-bias" tabindex="-1"><a class="header-anchor" href="#强-inductive-bias-vs-弱-inductive-bias"><span>强 inductive bias vs 弱 inductive bias</span></a></h3><table><thead><tr><th>类型</th><th>特征</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>强 bias</td><td>假设多</td><td>小数据表现好</td><td>灵活性差</td></tr><tr><td>弱 bias</td><td>假设少</td><td>表达能力强</td><td>需要大量数据</td></tr></tbody></table><p>CNN &gt; 强 bias<br> Transformer &gt; 弱 bias</p><p>这也是为什么大模型必须用海量数据训练。</p><hr><h3 id="从哲学角度理解" tabindex="-1"><a class="header-anchor" href="#从哲学角度理解"><span>从哲学角度理解</span></a></h3><p>Inductive bias 本质上是：</p><blockquote><p>模型对“世界如何运作”的结构性信念</p></blockquote><p>没有这种信念，泛化是不可能的。</p><p>这和人类学习是一样的——<br> 人类天生就带有强 inductive bias，比如：</p><ul><li>物体连续存在</li><li>相似形状有相似功能</li><li>语言具有结构</li></ul><hr><h3 id="更工程化的理解-对你这种做系统的人" tabindex="-1"><a class="header-anchor" href="#更工程化的理解-对你这种做系统的人"><span>更工程化的理解（对你这种做系统的人）</span></a></h3><p>如果用系统设计语言来讲：</p><p>Inductive bias = 你在架构层面 baked-in 的 inductive assumption。</p><p>比如：</p><ul><li>用卷积而不是全连接</li><li>用 position embedding</li><li>用 graph structure</li><li>用 physics prior</li></ul><p>这些不是数据学出来的，是设计时注入的。</p><p>这在你做引擎或系统架构时其实非常熟悉：<br> 你不可能写一个“完全通用”的系统，<br> 你总是 embed 某种 world model。</p><hr><h3 id="一个更抽象的总结" tabindex="-1"><a class="header-anchor" href="#一个更抽象的总结"><span>一个更抽象的总结</span></a></h3><p>Inductive bias = 限制 hypothesis space 的方式。</p><p>如果 hypothesis space 是：</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">H</mi></mrow><annotation encoding="application/x-tex"> \\mathcal{H} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathcal" style="margin-right:0.00965em;">H</span></span></span></span></p><p>那么 inductive bias 就是把它缩小成：</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="script">H</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>⊂</mo><mi mathvariant="script">H</mi></mrow><annotation encoding="application/x-tex"> \\mathcal{H}&#39; \\subset \\mathcal{H} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.791em;vertical-align:-0.0391em;"></span><span class="mord"><span class="mord mathcal" style="margin-right:0.00965em;">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">⊂</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathcal" style="margin-right:0.00965em;">H</span></span></span></span></p><p>或者给不同 hypothesis 不同优先级。</p><hr><p>如果你愿意，我可以进一步讲：</p><ul><li>inductive bias 在 deep learning scaling law 中的作用</li><li>为什么 foundation model 刻意弱化 inductive bias</li><li>或者从 PAC learning / VC 维角度推导它的必要性</li></ul>',76)]))}]]),p=JSON.parse('{"path":"/posts/AI/AI%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/inductive%20bias.html","title":"什么是inductive bias","lang":"zh-CN","frontmatter":{"date":"2026-02-25T00:00:00.000Z","tag":["AI","AI_GEN"],"description":"什么是inductive bias 在 AI / 机器学习语境中，**inductive bias（归纳偏置）**是指： 当训练数据不足以唯一确定一个函数时，模型为了能够做出泛化预测而“额外假设”的那部分结构性偏好。 更形式化一点： 设训练数据为 ( D = {(xi​,yi​)} ) 存在多个函数 ( f ) 都可以完美拟合 ( D ) 模型最终选择...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"什么是inductive bias\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2026-02-25T00:00:00.000Z\\",\\"dateModified\\":\\"2026-02-25T08:23:55.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Runner2011\\",\\"url\\":\\"https://runner2011.github.io\\"}]}"],["meta",{"property":"og:url","content":"https://runner2011.github.io/posts/AI/AI%E6%A6%82%E5%BF%B5%E5%85%A5%E9%97%A8/inductive%20bias.html"}],["meta",{"property":"og:site_name","content":"Runner2011 blog"}],["meta",{"property":"og:title","content":"什么是inductive bias"}],["meta",{"property":"og:description","content":"什么是inductive bias 在 AI / 机器学习语境中，**inductive bias（归纳偏置）**是指： 当训练数据不足以唯一确定一个函数时，模型为了能够做出泛化预测而“额外假设”的那部分结构性偏好。 更形式化一点： 设训练数据为 ( D = {(xi​,yi​)} ) 存在多个函数 ( f ) 都可以完美拟合 ( D ) 模型最终选择..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2026-02-25T08:23:55.000Z"}],["meta",{"property":"article:tag","content":"AI_GEN"}],["meta",{"property":"article:tag","content":"AI"}],["meta",{"property":"article:published_time","content":"2026-02-25T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2026-02-25T08:23:55.000Z"}]]},"git":{"createdTime":1772007835000,"updatedTime":1772007835000,"contributors":[{"name":"runner2011","username":"runner2011","email":"chenjfsea@gmail.com","commits":1,"url":"https://github.com/runner2011"}]},"readingTime":{"minutes":2.99,"words":898},"filePathRelative":"posts/AI/AI概念入门/inductive bias.md","excerpt":"\\n<p>在 AI / 机器学习语境中，**inductive bias（归纳偏置）**是指：</p>\\n<blockquote>\\n<p>当训练数据不足以唯一确定一个函数时，模型为了能够做出泛化预测而“额外假设”的那部分结构性偏好。</p>\\n</blockquote>\\n<p>更形式化一点：</p>\\n<ul>\\n<li>设训练数据为 ( D = {(<span v-pre=\\"\\" class=\\"katex\\"><span class=\\"katex-mathml\\"><math xmlns=\\"http://www.w3.org/1998/Math/MathML\\"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo separator=\\"true\\">,</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding=\\"application/x-tex\\">x_i, y_i</annotation></semantics></math></span><span class=\\"katex-html\\" aria-hidden=\\"true\\"><span class=\\"base\\"><span class=\\"strut\\" style=\\"height:0.625em;vertical-align:-0.1944em;\\"></span><span class=\\"mord\\"><span class=\\"mord mathnormal\\">x</span><span class=\\"msupsub\\"><span class=\\"vlist-t vlist-t2\\"><span class=\\"vlist-r\\"><span class=\\"vlist\\" style=\\"height:0.3117em;\\"><span style=\\"top:-2.55em;margin-left:0em;margin-right:0.05em;\\"><span class=\\"pstrut\\" style=\\"height:2.7em;\\"></span><span class=\\"sizing reset-size6 size3 mtight\\"><span class=\\"mord mathnormal mtight\\">i</span></span></span></span><span class=\\"vlist-s\\">​</span></span><span class=\\"vlist-r\\"><span class=\\"vlist\\" style=\\"height:0.15em;\\"><span></span></span></span></span></span></span><span class=\\"mpunct\\">,</span><span class=\\"mspace\\" style=\\"margin-right:0.1667em;\\"></span><span class=\\"mord\\"><span class=\\"mord mathnormal\\" style=\\"margin-right:0.03588em;\\">y</span><span class=\\"msupsub\\"><span class=\\"vlist-t vlist-t2\\"><span class=\\"vlist-r\\"><span class=\\"vlist\\" style=\\"height:0.3117em;\\"><span style=\\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\\"><span class=\\"pstrut\\" style=\\"height:2.7em;\\"></span><span class=\\"sizing reset-size6 size3 mtight\\"><span class=\\"mord mathnormal mtight\\">i</span></span></span></span><span class=\\"vlist-s\\">​</span></span><span class=\\"vlist-r\\"><span class=\\"vlist\\" style=\\"height:0.15em;\\"><span></span></span></span></span></span></span></span></span></span>)} )</li>\\n<li>存在多个函数 ( f ) 都可以完美拟合 ( D )</li>\\n<li>模型最终选择其中某一个</li>\\n<li>这个“选择机制”背后的偏好，就是 inductive bias</li>\\n</ul>","autoDesc":true}')}}]);