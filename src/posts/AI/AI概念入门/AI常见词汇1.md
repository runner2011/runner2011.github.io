---
tag:
  - AI_GEN

---
# AI常见词汇 (1) - 算力

## 什么是算力
“算力”是“计算能力”的简称，用来衡量计算系统（如CPU、GPU、TPU、AI芯片、超算集群等）处理任务的能力

## 算力怎么表示
🔢一般常见的算力单位
|单位|	全称|	用途举例|
| ---- | ---- | ---- |
FLOPS	|Floating Point Operations Per Second（每秒浮点运算次数）|	通用计算能力衡量（科学计算、AI推理等）
TOPS	|Tera Operations Per Second（每秒万亿次操作）	|常用于AI芯片（整数运算、低精度推理）
MIPS/GIPS	|Million/Billion Instructions Per Second（每秒百万/十亿条指令）	|常用于传统嵌入式/CPU性能测量
Hash/s	|Hashes per second（每秒哈希数）	|用于区块链/挖矿领域
TFLOPS/EFLOPS	|Tera/Exa FLOPS（每秒万亿/百亿亿次浮点运算）	|高性能计算（如超算、AI训练）衡量单位

假设一个GPU有：
单精度浮点性能为 10 TFLOPS，这意味着它每秒可以执行 10 × 10¹² 次浮点运算。

**设备算力对比表**
项目	|Apple A17 Pro|	Jetson Orin Nano	|Switch 2（推测）|	树莓派 5
| ---- | ---- | ---- | ---- | ---- |
架构	|ARM + NPU + GPU|	ARM + Ampere GPU|	ARM + Ampere GPU	|ARM Cortex-A76 (4核)
GPU|	自研 5核 + 光追|	1024 CUDA (Ampere)|	~1280 CUDA (Ampere)|	VideoCore VII
AI算力|	35 TOPS (NPU, INT8)	|40 TOPS (INT8)|	<10 TOPS（估计）|	❌ 无NPU（需靠CPU）
GPU浮点性能|	23 TFLOPS（估算）|	~1.5 TFLOPS FP32|	2~4 TFLOPS FP32|	~100 GFLOPS（估算）|
训练能力|	❌ 基本无	|⚠️ 仅极简小模型	|❌ 游戏平台，非训练用|	❌ CPU训练极慢
内存|	8 GB |LPDDR5	8~16 GB| LPDDR5	8~12 GB| LPDDR5X	4~8 GB| LPDDR4X
功耗|	~3W	|7~15W	|10~18W|	3~5W
定位|	移动端 |AI + 渲染|	边缘 AI 推理|	游戏主机	|教学、DIY、小型服务器


## 训练 vs 推理

🧠 目的不同
| 项目 | 训练（Training）        | 推理（Inference）       |
| -- | ------------------- | ------------------- |
| 目标 | 学习模型参数（权重）          | 使用学到的模型进行实际应用       |
| 功能 | 通过大量数据优化模型，使其具备预测能力 | 用训练好的模型对新数据进行预测、分类等 |

💻 计算资源需求
| 项目   | 训练                     | 推理                    |
| ---- | ---------------------- | --------------------- |
| 运算类型 | 以 FP32、FP16 为主（高精度浮点数） | 多用 INT8、FP16（可量化、低功耗） |
| 算力需求 | 极高：需大量矩阵乘法、梯度下降等复杂操作   | 较低：只需前向传播             |
| 硬件设备 | GPU/TPU/NPU/高性能服务器     | 手机、PC、嵌入式设备（也可在云上）    |
| 时间成本 | 长（可几小时到几周）             | 快速（通常毫秒级）             |

📉 表格总结（简洁版）
| 对比项    | 训练（Training） | 推理（Inference） |
| ------ | ------------ | ------------- |
| 是否更新权重 | ✅ 是          | ❌ 否           |
| 是否反向传播 | ✅ 有          | ❌ 无           |
| 数据类型   | 有标签大量数据      | 无标签新数据        |
| 运算精度   | 高（FP32、FP16） | 可低（INT8、FP16） |
| 硬件     | GPU/TPU      | 手机/边缘/云端服务器   |
| 速度     | 慢（小时\~周）     | 快（毫秒\~秒）      |
| 应用场景   | 模型构建         | 模型使用          |

### 为什么训练参数精度以F32,F16为主，推理以INT8,F16,INT4为主
训练梯度微小，需精度，支持反向传播。推理速度优先，可量化，训练后部署。


## 什么是AI算力
AI算力主要指处理AI计算的能力，一般通过"人工智能加速模块"实现
P.S. AI计算，有大量的矩阵乘加法(MAC)操作,比如CNN卷积操作。 所以提供AI算力要针对性的设计矩阵成加法计算单元。

## NPU是什么
- NPU = 神经网络处理器，专为加速深度学习模型（尤其是推理）设计。
- 通常支持 INT8/INT4/FP16 等低精度运算，每秒可处理数十亿~万亿次操作（TOPS）。
- 手机的 Apple Neural Engine、华为昇腾 NPU、NVIDIA Tensor Core、Google Edge TPU 都属于此类

### 
📌 一、为什么通用 CPU/GPU 不适合高效 AI 推理

#### 💡 AI推理的核心特征：

* 主要是 **矩阵乘法**（如 `Y = W·X + B`）
* 大量的 **乘加（MAC）操作**，而不是通用逻辑运算
* 并不要求很高的浮点精度（不像科学计算）

#### 🚫 CPU 问题：

* CPU 是通用控制器 + 大 cache，**适合分支跳转、系统调用等**控制逻辑
* 缺乏大量并行计算单元
* SIMD 指令支持 INT8/FP16 也有限

#### ⚠️ GPU 虽好，但仍有问题：

* 虽然支持浮点并行，但：

  * 传统 GPU 优化的是 **图形流水线**，不是矩阵乘法
  * 对 AI 推理的低精度（INT8/INT4）支持弱（过去时代）
  * 功耗高，不适合边缘设备

---

### ✅ 二、NPU 是怎么为 AI 定制的？（从架构出发）

NPU（Neural Processing Unit）的核心设计思想是：

> “将 AI 中的大量矩阵乘加（MAC）操作，用专门的硬件阵列并行处理，并支持低精度数据格式以提升吞吐和节能。”

---

### 🧱 三、NPU 的硬件结构层次图

我们来看看典型的 NPU 结构：

```
          ┌──────────────────────────────┐
          │         控制模块（FSM）       │
          └──────────────────────────────┘
                     │
          ┌──────────────────────────────┐
          │  调度 / 指令解码 / 流水调度   │
          └──────────────────────────────┘
                     │
          ┌──────────────────────────────┐
          │   Compute Core（MAC阵列）    │ <─ 支持 INT8/INT4/FP16
          └──────────────────────────────┘
               │      │       │
     ┌─────────┘      │       └────────────┐
     ▼                ▼                    ▼
Local SRAM      Weight Buffer        Activation Buffer
(支持片上访问)  (卷积核)            (输入特征图)

```

---

### ⚙️ 四、MAC 单元：核心计算引擎

#### ✅ MAC = Multiply + Accumulate

例如，INT8 推理时的卷积操作：

$$
 \mathrm{Output}[i] = \sum_j W[i,j] \cdot X[j] 
$$

NPU 内部会集成成 **数千\~上万个 MAC 单元**，组成 **矩阵计算阵列（如 systolic array）**。

🔧 典型布局举例：

* 64×64 MAC 阵列 ⇒ 一次可并行完成 `64×64` 的矩阵乘法
* 每个 MAC 支持 INT8 乘法 + INT32 累加（称为 INT8 → INT32）

---

### 🧮 五、为什么使用 INT8、INT4、FP16 而不是 FP32？

| 数据格式          | 位数 | 优点         | 缺点        |
| ------------- | -- | ---------- | --------- |
| **FP32**      | 32 | 高精度，兼容性强   | 慢，功耗高，面积大 |
| **FP16/BF16** | 16 | 更省资源，适合训练  | 精度可能不足    |
| **INT8**      | 8  | 非常高效，推理常用  | 精度要靠量化保证  |
| **INT4**      | 4  | 极限压缩，需专门算法 | 精度损失较大    |

📉 精度 vs 资源消耗示意：

```
FP32  > FP16  > INT8  > INT4
  ↑         ↑        ↑        ↑
最耗资源                    最省资源
```

---

### 📏 六、算力单位如何体现这个低精度特性？

#### 单位回顾：

* **1 FLOPS**：一次 FP32 运算（加、乘）
* **1 TOPS**：一次整数操作（通常是 INT8 MAC）
* 所以：

> **同样硬件面积下，INT8 运算的并行度比 FP32 高 4 倍以上**
> INT4 并行度更高 → 可达到更高 TOPS

---

### 🔁 七、NPU的“低精度+高并行”如何发挥巨大优势？

举个例子：

* 面积单位下，你可以放：

  * 1 个 FP32 乘法器
  * 或 4 个 FP16
  * 或 16 个 INT8
  * 或 32 个 INT4

所以：

> 同一片面积和功耗预算下，NPU 可以用低精度堆出超高吞吐量（TOPS），远超 CPU/GPU

这就是为什么：

* 手机 NPU（A17 Pro）能达 **35 TOPS**
* Jetson Orin Nano 能达 **40 TOPS**
* 而同面积的 CPU/GPU 无法接近这种密度

---

### 🔄 八、为什么训练更偏好 FP32/FP16，而推理偏好 INT8/INT4？

| 阶段     | 偏好精度             | 原因              |
| ------ | ---------------- | --------------- |
| **训练** | FP32 → FP16/BF16 | 梯度微小，需精度，支持反向传播 |
| **推理** | INT8 / INT4      | 速度优先，可量化，训练后部署  |

---

### ✅ 九、总结一句话：

> **NPU 是为神经网络中的大规模矩阵乘加而生的专用加速器，采用低精度（INT8/INT4/FP16）+ 大规模 MAC 并行阵列，实现高吞吐、低功耗的 AI 算力。**
