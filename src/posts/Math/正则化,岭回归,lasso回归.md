---
date: 2025-07-08
tag:
  - Math
  - AI_GEN

---

# 正则化,岭回归,lasso回归概念

A:把我当作退休的老太太，给我解释正则化，岭回归，lasso回归。

好！我来用退休老太太能听懂的方式，慢慢给你讲讲哈。我们就当在小区凉亭里唠嗑，慢慢来：

---

## 🌸 一、先说说**正则化**是个啥

你做菜放盐吧，太多太少都不好。模型也是这样，学得太少，预测不准；学得太多，连你家狗叫声的节奏都学进去，结果到新数据上就翻车，这叫**过拟合**。

**正则化**就是：
👉 在模型学数据的时候，旁边盯着，别让你学得太复杂了。
就像做菜有人提醒：“哎哎，盐别放太多，别太咸了哈！”

用数学说法：

> **在原本的损失函数后面，加个惩罚项**，约束模型参数（就是公式里的系数）别乱跑太大。

---

## 🌸 二、岭回归（Ridge Regression）

你家有好几个儿子，每个儿子挣得多挣得少都不同。你给他们发压岁钱，怕有个别儿子要太多。

**岭回归**就像：
👉 给每个系数（儿子）发压岁钱，要求：

* 总金额不能太大。
* 谁也别太夸张。

方法是：
在损失函数后面加上：

> **所有系数的平方和 × λ**

λ 是个你定的参数，λ 越大，管得越严，大家拿得都差不多，谁也别太离谱。

数学长这样：

$$
损失 = 原来的误差 + λ \times (所有系数的平方和)
$$

特点：

* 把所有系数往 0 靠，但都不会变成 0。
* 用来防止过拟合，尤其是特征（变量）很多的时候。

---

## 🌸 三、Lasso回归（Least Absolute Shrinkage and Selection Operator）

Lasso 就更狠了，不仅管你别太大，还能直接把某些儿子从压岁钱名单上划掉，不给了！

方法是：
在损失函数后面加上：

> **所有系数的绝对值和 × λ**

数学长这样：

$$
损失 = 原来的误差 + λ \times (所有系数的绝对值和)
$$

特点：

* λ 大的时候，某些系数会被压到 0，等于删掉这个变量。
* 适合做**特征筛选**，选出对预测最有用的那几个。

---

## 🌸 总结一下：

| 名字       | 方式          | 作用               |
| :------- | :---------- | :--------------- |
| 正则化      | 惩罚项防止模型太复杂  | 防止过拟合            |
| 岭回归      | 加**平方和**惩罚  | 把系数都往 0 靠，但不变成 0 |
| Lasso 回归 | 加**绝对值和**惩罚 | 有的系数直接压成 0，等于删变量 |

---

## 🌸 最后老太太总结：

哎呀，这不就跟我们家发压岁钱似的嘛：

* 怕有的孩子张嘴要太多，得管着点（正则化）
* 谁都别要太离谱（岭回归）
* 有些不争气的，干脆别给了（Lasso）

---

要是你喜欢，我还能再帮你画个图，或者用生活故事多比喻几个场景 👵✨ 要不要？
